{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwfDoyu83mnUfcOqNaic6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvathysarat/kg-qa/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D55OR2Mm2KQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestionAnswering(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, entity_weights,rel_weights):\n",
        "        super(QuestionAnswering, self).__init__()\n",
        "\n",
        "        # entity embedding\n",
        "        self.entity_embeddings = nn.Embedding.from_pretrained(self.entity_weights)  \n",
        "        self.entity_embeddings.weight.requires_grad = False  \n",
        "        # word embedding dimension = 50\n",
        "        self.entity_linear = nn.Linear(in_features = 50,out_features = 50)\n",
        "\n",
        "        # relation embedding\n",
        "        self.relation_embeddings = nn.Embedding.from_pretrained(self.rel_weights)  \n",
        "        self.relation_embeddings.weight.requires_grad = False          \n",
        "        # entity dimension = 50\n",
        "        word_dim, entity_dim = 50\n",
        "        self.num_vocab = 13545\n",
        "        self.relation_linear = nn.Linear(in_features = 2*word_dim, out_features = entity_dim)\n",
        "        self.k=3\n",
        "        for i in range(3):\n",
        "            self.add_module('q2e_linear' + str(i), nn.Linear(in_features=entity_dim, out_features=entity_dim))\n",
        "            self.add_module('d2e_linear' + str(i), nn.Linear(in_features=entity_dim, out_features=entity_dim))\n",
        "            self.add_module('e2q_linear' + str(i), nn.Linear(in_features=self.k * entity_dim, out_features=entity_dim))\n",
        "            self.add_module('e2d_linear' + str(i), nn.Linear(in_features=self.k * entity_dim, out_features=entity_dim))\n",
        "            self.add_module('e2e_linear' + str(i), nn.Linear(in_features=self.k * entity_dim, out_features=entity_dim))\n",
        "            \n",
        "        \n",
        "            self.add_module('kb_head_linear' + str(i), nn.Linear(in_features=entity_dim, out_features=entity_dim))\n",
        "            self.add_module('kb_tail_linear' + str(i), nn.Linear(in_features=entity_dim, out_features=entity_dim))\n",
        "            self.add_module('kb_self_linear' + str(i), nn.Linear(in_features=entity_dim, out_features=entity_dim))\n",
        "\n",
        "        # initialize document embeddings\n",
        "        \n",
        "        self.doc_embedding = nn.Embedding(num_embeddings = num_vocab+1, embedding_dim = word_dim, padding_idx = sef.num_vocab)  \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,dropout = DROPOUT, num_layers = LSTM_LAYERS, bidirection=True)\n",
        "\n",
        "        # create LSTMs\n",
        "        self.node_encoder = nn.LSTM(input_size = word_dim, hidden_size = entity_dim, batch_first = True)\n",
        "        # self.q_rel_encoder = nn.LSTM(input_size = word_dim, hidden_size = entity_dim, batch_first = True)\n",
        "        self.bi_doc_encoder = nn.LSTM(input_size = word_dim, hidden_size = entity_dim, batch_first = True, bidirectional=True)\n",
        "        self.doc_info_carrier = nn.LSTM(input_size = entity_dim, hidden_size = entity_dim, batch_first = True,bidirectional=True)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # dropout\n",
        "        self.lstm_drop = nn.Dropout(p=lstm_dropout)\n",
        "        self.linear_drop = nn.Dropout(p=linear_dropout)\n",
        "\n",
        "        self.score_func = nn.Linear(in_features=entity_dim, out_features=1)\n",
        "        \n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.softmax_d1 = nn.Softmax(dim=1)\n",
        "        self.kld_loss = nn.KLDivLoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        self.bce_loss_logits = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        tag_scores = None\n",
        "        # Implement the forward pass.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM-m8H962XaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PVWgBPd2YIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}