{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataloader.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNk5JQAtjPn7vpC1EiSjQCZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvathysarat/kg-qa/blob/master/dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIYFJJ-EUs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_dict(filename):\n",
        "    word2id = dict()\n",
        "    with open(filename) as f_in:\n",
        "        for line in f_in:\n",
        "            word = line.strip().decode('UTF-8')\n",
        "            word2id[word] = len(word2id)\n",
        "    return word2id\n",
        "\n",
        "class DataLoader():\n",
        "  def __init__(self, documents,data, doc_entity_index, all_doc_texts, vocab_ids, relation_ids, entity_ids):\n",
        "    self.documents = documents\n",
        "    self.doc_entity_index = doc_entity_index \n",
        "    self.all_doc_texts = all_doc_texts \n",
        "    self.vocab_ids = vocab_ids\n",
        "    self.relation_ids = relation_ids\n",
        "    self.entity_ids = entity_ids\n",
        "    self.ids_entity = {id:entity for entity, id in entity_ids.items()}\n",
        "    self.data = []\n",
        "    self.max_rel_doc, self.max_facts = 0,0\n",
        "    with open(data) as f:\n",
        "      for line in f:\n",
        "        line = json.loads(line)\n",
        "        self.data.append(line)\n",
        "        self.max_rel_doc = max(self.max_rel_doc, len(line['passages']))\n",
        "        self.max_facts = max(self.max_facts, 2*len(line['docs']['tuples']))\n",
        "    self.num_data = len(self.data)\n",
        "    self.batches = np.arange(self.num_data)\n",
        "    # build entity maps\n",
        "    self.entity_maps = self.entity_maps()\n",
        "    self.max_local_entity, self.max_facts = 0,0\n",
        "    self.num_kb_relation = len(relation_ids)\n",
        "    self.max_query_word = 10\n",
        "    self.local_entities = np.full((self.num_data, self.max_local_entity), len(self.entity_ids), dtype=int)\n",
        "    self.kb_adj_mats = np.empty(self.num_data, dtype=object)\n",
        "    self.kb_fact_rels = np.full((self.num_data, self.max_facts), self.num_kb_relation, dtype=int)\n",
        "    self.q2e_adj_mats = np.zeros((self.num_data, self.max_local_entity, 1), dtype=float)\n",
        "    self.query_texts = np.full((self.num_data, self.max_query_word), len(self.vocab_ids), dtype=int)\n",
        "    self.rel_document_ids = np.full((self.num_data, self.max_relevant_doc), -1, dtype=int) # the last document is empty\n",
        "    self.entity_poses = np.empty(self.num_data, dtype=object)\n",
        "    self.answer_dists = np.zeros((self.num_data, self.max_local_entity), dtype=float)\n",
        "    \n",
        "    self.prepare_data()\n",
        "\n",
        "  def prepare_data(self):\n",
        "    self.use_inverse_relation = False\n",
        "    next_id = 0\n",
        "    count_query_length = [0] * 50\n",
        "    total_num_answerable_question = 0\n",
        "    for sample in tqdm(self.data):\n",
        "        # get a list of local entities\n",
        "        global_to_local = self.entity_ids[next_id]\n",
        "        for global_entity, local_entity in global_to_local.items():\n",
        "            if local_entity != 0: # skip question node\n",
        "                self.local_entities[next_id, local_entity] = global_entity\n",
        "\n",
        "        entity2fact_e, entity2fact_f = [], []\n",
        "        fact2entity_f, fact2entity_e = [], []\n",
        "\n",
        "        entity_pos_local_entity_id = []\n",
        "        entity_pos_word_id = []\n",
        "        entity_pos_word_weights = []\n",
        "\n",
        "        # relations in local KB\n",
        "        \n",
        "        for i, tpl in enumerate(sample['docs']['tuples']):\n",
        "            sbj, rel, obj = tpl\n",
        "            if not self.use_inverse_relation:\n",
        "                entity2fact_e += [g2l[self.entity2id[sbj['text']]]]\n",
        "                entity2fact_f += [i]\n",
        "                fact2entity_f += [i]\n",
        "                fact2entity_e += [g2l[self.entity2id[obj['text']]]]\n",
        "                self.kb_fact_rels[next_id, i] = self.relation2id[rel['text']]\n",
        "                \n",
        "        # build connection between question and entities in it\n",
        "        for j, entity in enumerate(sample['entities']):\n",
        "            self.q2e_adj_mats[next_id, g2l[self.entity_ids[unicode(entity['text'])]], 0] = 1.0\n",
        "\n",
        "        # connect documents to entities occurred in it\n",
        "        \n",
        "        for j, passage in enumerate(sample['passages']):\n",
        "            document_id = passage['document_id']                \n",
        "            if document_id not in self.doc_entity_index:\n",
        "                continue\n",
        "            (global_entity_ids, word_ids, word_weights) = self.doc_entity_index[document_id]\n",
        "            entity_pos_local_entity_id += [global_to_local[global_entity_id] for global_entity_id in global_entity_ids]\n",
        "            entity_pos_word_id += [word_id + j * self.max_document_word for word_id in vocab_ids]\n",
        "            entity_pos_word_weights += word_weights\n",
        "\n",
        "        # tokenize question\n",
        "        count_query_length[len(sample['question'].split())] += 1\n",
        "        for j, word in enumerate(sample['question'].split()):\n",
        "            if j < self.max_query_word:\n",
        "                if word in self.vocab_ids:\n",
        "                    self.query_texts[next_id, j] = self.vocab_ids[word]\n",
        "                else: \n",
        "                    self.query_texts[next_id, j] = self.vocab_ids['__unk__']\n",
        "\n",
        "        # tokenize document\n",
        "        for pid, passage in enumerate(sample['passages']):\n",
        "            self.rel_document_ids[next_id, pid] = passage['document_id']\n",
        "\n",
        "        # construct distribution for answers\n",
        "        for answer in sample['answers']:\n",
        "            keyword = 'text' if type(answer['kb_id']) == int else 'kb_id'\n",
        "            if self.entity_ids[answer[keyword]] in g2l:\n",
        "                self.answer_dists[next_id, global_to_local[self.entity_ids[answer[keyword]]]] = 1.0\n",
        "\n",
        "        self.kb_adj_mats[next_id] = (np.array(entity2fact_f, dtype=int), np.array(entity2fact_e, dtype=int), np.array([1.0] * len(entity2fact_f))), (np.array(fact2entity_e, dtype=int), np.array(fact2entity_f, dtype=int), np.array([1.0] * len(fact2entity_e)))\n",
        "        self.entity_poses[next_id] = (entity_pos_local_entity_id, entity_pos_word_id, entity_pos_word_weights)\n",
        "        \n",
        "        next_id += 1    \n",
        "\n",
        "\n",
        "\n",
        "  def add_entity_to_map(entity_ids, entities, global_to_local):\n",
        "    for ent in entities:\n",
        "      ent_text = ent['text']\n",
        "      ent_g_id = entity_ids[ent_text]\n",
        "      if ent_g_id not in global_to_local:\n",
        "        global_to_local[entity_ids[ent_text]] = len(global_to_local)\n",
        "\n",
        "  def entity_maps(self):\n",
        "    entity_maps = [None]*self.num_data\n",
        "    total_local_entity =0.0\n",
        "    next_id = 0\n",
        "    for data in self.data:\n",
        "      global_to_local = {}\n",
        "      self.add_entity_to_map(self.entity_ids, data['entities'],global_to_local)\n",
        "      self.add_entity_to_map(self.entity_ids, data['docs']['entities'], global_to_local)\n",
        "      for doc in data['passages']:\n",
        "        if doc['document_id'] not in self.documents: continue\n",
        "        document = self.documents[int(doc['document_id'])]\n",
        "        self.add_entity_to_map(self.entity_ids, doc['document']['entities'], global_to_local)\n",
        "        if 'title' in doc: self.add_entity_to_map(self.entity_ids, doc['title']['entities'], global_to_local)\n",
        "\n",
        "        entity_maps[next_id] = global_to_local\n",
        "        total_local_entity += len(global_to_local)\n",
        "        self.max_local_entity = max(self.max_local_entity, len(global_to_local))\n",
        "        next_id += 1\n",
        "\n",
        "    return entity_maps\n",
        "          \n",
        "    def build_entity_pos(self, sample_ids):\n",
        "        \"\"\"Index the position of each entity in documents\"\"\"\n",
        "        entity_pos_batch = np.array([], dtype=int)\n",
        "        entity_pos_entity_id = np.array([], dtype=int)\n",
        "        entity_pos_word_id = np.array([], dtype=int)\n",
        "        vals = np.array([], dtype=float)\n",
        "\n",
        "        for i, sample_id in enumerate(sample_ids):\n",
        "            (entity_id, word_id, val) = self.entity_poses[sample_id]\n",
        "            num_nonzero = len(val)\n",
        "            entity_pos_batch = np.append(entity_pos_batch, np.full(num_nonzero, i, dtype=int))\n",
        "            entity_pos_entity_id = np.append(entity_pos_entity_id, entity_id)\n",
        "            entity_pos_word_id = np.append(entity_pos_word_id, word_id)\n",
        "            vals = np.append(vals, val)\n",
        "        return (entity_pos_batch.astype(int), entity_pos_entity_id.astype(int), entity_pos_word_id.astype(int), vals)\n",
        "\n",
        "    def build_kb_adj_mat(self, sample_ids, fact_dropout):\n",
        "\n",
        "        mats0_batch = np.array([], dtype=int)\n",
        "        mats0_0 = np.array([], dtype=int)\n",
        "        mats0_1 = np.array([], dtype=int)\n",
        "        vals0 = np.array([], dtype=float)\n",
        "\n",
        "        mats1_batch = np.array([], dtype=int)\n",
        "        mats1_0 = np.array([], dtype=int)\n",
        "        mats1_1 = np.array([], dtype=int)\n",
        "        vals1 = np.array([], dtype=float)\n",
        "\n",
        "        for i, sample_id in enumerate(sample_ids):\n",
        "            (mat0_0, mat0_1, val0), (mat1_0, mat1_1, val1) = self.kb_adj_mats[sample_id]\n",
        "            assert len(val0) == len(val1)\n",
        "            num_fact = len(val0)\n",
        "            num_keep_fact = int(np.floor(num_fact * (1 - fact_dropout)))\n",
        "            mask_index = np.random.permutation(num_fact)[ : num_keep_fact]\n",
        "            # mat0\n",
        "            mats0_batch = np.append(mats0_batch, np.full(len(mask_index), i, dtype=int))\n",
        "            mats0_0 = np.append(mats0_0, mat0_0[mask_index])\n",
        "            mats0_1 = np.append(mats0_1, mat0_1[mask_index])\n",
        "            vals0 = np.append(vals0, val0[mask_index])\n",
        "            # mat1\n",
        "            mats1_batch = np.append(mats1_batch, np.full(len(mask_index), i, dtype=int))\n",
        "            mats1_0 = np.append(mats1_0, mat1_0[mask_index])\n",
        "            mats1_1 = np.append(mats1_1, mat1_1[mask_index])\n",
        "            vals1 = np.append(vals1, val1[mask_index])\n",
        "\n",
        "        return (mats0_batch, mats0_0, mats0_1, vals0), (mats1_batch, mats1_0, mats1_1, vals1)\n",
        "\n",
        "\n",
        "    def reset_batches(self, is_sequential=True):\n",
        "        if is_sequential:\n",
        "            self.batches = np.arange(self.num_data)\n",
        "        else:\n",
        "            self.batches = np.random.permutation(self.num_data)\n",
        "\n",
        "\n",
        "    def get_batch(self, iteration, batch_size, fact_dropout):\n",
        "\n",
        "        sample_ids = self.batches[batch_size * iteration: batch_size * (iteration + 1)]\n",
        "        \n",
        "        return self.local_entities[sample_ids], \\\n",
        "               self.q2e_adj_mats[sample_ids], \\\n",
        "               (self.build_kb_adj_mat(sample_ids, fact_dropout=fact_dropout)), \\\n",
        "               self.kb_fact_rels[sample_ids], \\\n",
        "               self.query_texts[sample_ids], \\\n",
        "               self.build_document_text(sample_ids), \\\n",
        "               (self.build_entity_pos(sample_ids)), \\\n",
        "               self.answer_dists[sample_ids]\n",
        "\n",
        "\n",
        "    def build_document_text(self, sample_ids):\n",
        "        \"\"\"Index tokenized documents for each sample\"\"\"\n",
        "        document_text = np.full((len(sample_ids), self.max_relevant_doc, self.max_document_word), len(self.word2id), dtype=int)\n",
        "        for i, sample_id in enumerate(sample_ids):\n",
        "            for j, rel_doc_id in enumerate(self.rel_document_ids[sample_id]):\n",
        "                if rel_doc_id not in self.document_texts:\n",
        "                    continue\n",
        "                document_text[i, j] = self.document_texts[rel_doc_id]\n",
        "        return document_text\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLFg047ieGa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}